{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note of Hands-on ML with Scikit-Learn & TensorFlow by Lei Fu\n",
    "\n",
    "## Chapter 1. The Machine Learning Landscape\n",
    "\n",
    "> Early specialized applications, e.g. Optical Character Recognition (OCR). Became main stream in 1990s: _spam filter_.\n",
    "\n",
    "### What Is Machine Learning?\n",
    "\n",
    "Machine\tLearning is\tthe\tscience\t(and art) of programming computers so they can _learn_ from data.\n",
    "\n",
    "Machine\tLearning is the\tfield of study that gives computers\tthe\tability\tto learn without being explicitly programmed.\n",
    "<div style=\"text-align: right\"> - Arthur Samuel, 1959 </div> \n",
    "\n",
    "A computer program is said to learn\tfrom experience\tE with respect to some task T and some performance measure P, if\tits\tperformance\ton T, as measured by P, improves with experience E.\n",
    "<div style=\"text-align: right\"> - Tom Mitchell, 1997 </div> \n",
    "\n",
    "Training set, training instance (sample), accuracy\n",
    "\n",
    "### Why Use Machine Learning \n",
    "\n",
    "<div style=\"width:600 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig1-1.png\" width=600px alt=\"fig1-1\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 1-1. The traditional approach_</div>\n",
    "\n",
    "<div style=\"width:600 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig1-2.png\" width=600px alt=\"fig1-2\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 1-2. Machine Learning approach_</div>\n",
    "\n",
    "<div style=\"width:600 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig1-3.png\" width=600px alt=\"fig1-3\" style=\"padding-bottom:1.0em;padding-top:2.0em\"></center>_Figure 1-3. Automatically adapting to change_</div>\n",
    "\n",
    "<div style=\"width:600 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig1-4.png\" width=600px alt=\"fig1-4\" style=\"padding-bottom:1.0em;padding-top:2.0em\"></center>_Figure 1-4. Machine learning can help humans learning_</div>\n",
    "\n",
    "Machine\tLearning is great for:\n",
    "1. Problems\tfor which existing solutions require a lot of hand-tuning or long lists of rules: one Machine Learning algorithm can often simplify code and perform better (shorter, easier to maintain, accurate).\n",
    "2. Complex problems for traditional approaches or have no known algorithm (speech recognition).\n",
    "3. Fluctuating environments: a Machine Learning\tsystem can adapt to new data.\n",
    "4. Help humans learning. Getting insights about complex problems and large amounts of data.\n",
    "\n",
    "### Types of Machine Learning Systems\n",
    "\n",
    "Amount and type of human supervision:\n",
    "1. supervised\n",
    "2. unsupervised\n",
    "3. semisupervised\n",
    "4. reinforcement learning\n",
    "<br>\n",
    "\n",
    "Whether or not they can learn incrementally on the fly:\n",
    "1. online\n",
    "2. batch learning\n",
    "\n",
    "Whether they work by simply comparing new data points to known data points, or instead detect patterns in the training data and build a predictive model:\n",
    "1. instance-based\n",
    "2. model-based\n",
    "\n",
    "#### Supervised/Unsupervised Learning\n",
    "\n",
    "__Supervised Learning__\n",
    "\n",
    "Label: desired solution\n",
    "\n",
    "Classification/regression, predictors. \n",
    "<br>\n",
    "_Logistic Regression_: regression algorithm sued for classification. \n",
    "<br>\n",
    "Attribute: a data type; Feature: attribute + its value\n",
    "\n",
    "Supervised learning algorithms:\n",
    "1. k-Nearest Neighbors\n",
    "2. Linear Regression\n",
    "3. Logistic Regression\n",
    "4. Support Vector Machines (SVM)\n",
    "5. Decision Tress and Random Forests\n",
    "6. Neural networks\n",
    "\n",
    "__Unsupervised Learning__\n",
    "Training data is unlabeled.\n",
    "\n",
    "Unsupervised learning algorithms:\n",
    "\n",
    "Clustering\n",
    "1. k-Means\n",
    "2. Hierarchical Cluster Analysis (HCA)\n",
    "3. Expectation Maximization\n",
    "\n",
    "Visualization and dimensionality reduction\n",
    "1. Principal Component Analysis (PCA)\n",
    "2. Kernel PCA\n",
    "3. Locally-Linear Embedding (LLE)\n",
    "4. t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Association rule learning\n",
    "-  Apriori\n",
    "-  Eclat\n",
    "\n",
    "Visualization, dimensionality reduction, feature extraction. Reduce dimension of training data before feed to ML algorithm, benefit: run faster, less disk and memory usage, sometimes better performance.\n",
    "\n",
    "_Anomaly detection_\n",
    "\n",
    "_Association rule learning_\n",
    "<br>\n",
    "Goal: dig into large amounts of data and discover interesting relations\tbetween\tattributes.\n",
    "\n",
    "__Semisupervised Learning__\n",
    "<br>\n",
    "Algorithm deals with partially labeled training data (usually a\tlot of unlabeled data and a little bit of labeled data).\n",
    "\n",
    "__Reinforcement Learning__\n",
    "<br>\n",
    "The\t_learning system_, called an <font color=blue>_agent_</font> in\tthis context, can observe the environment, select and perform actions, and\tget\trewards\tin return (or penalties\tin the form of negative rewards). It must then learn by itself what is the best strategy, called a _policy_, to get the most reward over time. A policy defines what action the\tagent should choose when it\tis in a\tgiven situation. \n",
    "\n",
    "#### Batch and Online Learning\n",
    "\n",
    "__Batch learning__ (_offline Learning_) \n",
    "<br>\n",
    "The system is incapable of learning incrementally: it must be trained using all the available data. Taking a lot of time and computing resources, so done _offline_.\n",
    "\n",
    "(Time) Not able to adapt to rapidly changing data; (computing resource) expensive; not able to learn autonomously and limited resources.\n",
    "\n",
    "__Online learning__ (_incremental learning_)\n",
    "<br>\n",
    "Train the system incrementally by feeding it data instances sequentially, either individually or by small groups called\tmini-batches. (Each\tlearning step is fast and cheap, so\tthe\tsystem can learn about new data on the fly,\tas it arrives.\n",
    "\n",
    "<div style=\"width:800 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig1-13.png\" width=800px alt=\"fig1-13\" style=\"padding-bottom:1.0em;padding-top:2.0em\"></center>_Figure 1-13. Online learning_</div>\n",
    "\n",
    "Ideal for system:\n",
    "- receive data as a continuous flow\n",
    "- need to adapt to change rapidly or autonomously\n",
    "- limited resources (discard old data, save space)\n",
    "- _out-of-core learning_ (huge data that cannot fit in memory)\n",
    "\n",
    "<font color=red>_WARNING_</font>:\n",
    "<br>\n",
    "This whole process is usually done offline (i.e., not on the live system), so online learning can be a confusing name.\tThink of it as incremental learning.\n",
    "\n",
    "_Learning rate_: how fast to adapt to changing data.\n",
    "\n",
    "Challenge: if bad data is fed to the system, its performance will gradually decline. To reduce this risk you need to monitor your system closely and promptly switch learning off (and possibly revert to a previously working state) if you detect a drop in performance. You may also want to monitor the input data and react to abnormal data (anomaly detection).\n",
    "\n",
    "#### Supervised/Unsupervised Learning\n",
    "\n",
    "One\tmore way to categorize Machine Learning\tsystems is by how they _generalize_.\n",
    "\n",
    "__Instance-based learning__\n",
    "<br>\n",
    "The system learns the examples by heart, then generalizes to new cases using a _similarity measure_.\n",
    "\n",
    "__Model-based learning__\n",
    "<br>\n",
    "Another\tway\tto generalize from a set of\texamples is to build a model of these examples,\tthen use that model to make _predictions_.\n",
    "\n",
    "Utility\tfunction (or fitness function) measures\thow good your model is.\n",
    "<br>\n",
    "Cost function measures how bad it is.\n",
    "\n",
    "Linear\tRegression\talgorithm: you feed it your training examples and it finds the parameters that make the linear model fit best to your data.\tThis is\tcalled _training the model_.\n",
    "\n",
    "In\tsummary:\n",
    " - Study the data.\n",
    " - Select a model.\n",
    " - Train it on the training data (i.e., the learning algorithm searches\tfor the model parameter values that minimize a cost\tfunction).\n",
    " - Finally,\tyou\tapplied\tthe\tmodel to make predictions on new cases (this is called inference), hoping that this model will generalize well.\n",
    "\n",
    "### Main Challenges of Machine Learning\n",
    "\n",
    "Bad data or bad algorithm.\n",
    "\n",
    "#### Insufficient Quantity of Training Data\n",
    "\n",
    "Very different Machine Learning algorithms,\tincluding fairly simple\tones, performed\talmost identically well on a complex problem of natural language disambiguation\tonce they were given enough\tdata (Michele Banko\tand Eric Brill, 2001)\n",
    "\n",
    "The\tidea that data matters more\tthan algorithms\tfor\tcomplex\tproblems was further popularized by Peter Norvig et\tal in 2009 (“The Unreasonable Effectiveness of Data”). It should be noted, however, that small-\tand\tmedium-sized datasets are\n",
    "still very common, and it is not always\teasy or cheap to get extra training\tdata, so don’t abandon algorithms just yet.\n",
    "\n",
    "#### Nonrepresentative Training Data\n",
    "\n",
    "In order to generalize well, it\tis crucial that your training data be representative of the\tnew\tcases you want to generalize to.\n",
    "\n",
    "Difficulties:\n",
    " - If the sample is too\tsmall, you will have _sampling noise_ (i.e., nonrepresentative data as a result of chance).\n",
    " - Even\tvery large samples can be nonrepresentative\tif the sampling method is flawed. This is called _sampling bias_.\n",
    " \n",
    "#### Poor-Quality Data\n",
    "\n",
    "Errors,\toutliers, and noise require a significant mount of time to clean.\n",
    " - Obvious outliers, simply discard them or fix the errors manually.\n",
    " - If some instances are missing a few features, decide ignore this attribute, ignore these instances, fill in the missing values, or train one model with the feature and another without it.\n",
    " \n",
    "#### Irrelevant Features\n",
    "\n",
    "A critical part of the success of a Machine\tLearning project is\tcoming up with a good set of features to train on.\tThis process, called _feature engineering_, involves:\n",
    " - _Feature selection_: selecting the\tmost useful features to train on among existing features.\n",
    " - _Feature\textraction_: combining existing features to produce a more useful one (dimensionality reduction).\n",
    " - Creating\tnew\tfeatures by gathering new data.\n",
    "\n",
    "#### Overfitting the Training Data\n",
    "\n",
    "_Overfitting_:\tthe\tmodel performs well on the training\tdata, but it does not generalize well.\n",
    "\n",
    "<font color=red>_WARNING_</font>:\n",
    "<br>\n",
    "Overfitting\thappens\twhen the model is too complex relative to the amount and noisiness of the training data.\n",
    "\n",
    "Solutions:\n",
    " - Simplify the\tmodel by selecting one with fewer parameters, by reducing the number of attributes in the training\tdata or\tby constraining\tthe\tmodel (_regularization_)\n",
    " - Gather more training\tdata\n",
    " - Reduce the noise in the training\tdata (e.g.,\tfix\tdata errors\tand\tremove\toutliers)\n",
    " \n",
    "_Hyperparameter_: controls the amount of regularization to apply during learning.\n",
    "\n",
    "#### Underfitting the Training Data\n",
    "\n",
    "_Underfitting_: the model is too simple to learn the underlying\tstructure of the data.\t\n",
    "\n",
    "Solutions:\n",
    " - Selecting a more powerful model,\twith more parameters\n",
    " - Feeding better features to the learning algorithm (feature engineering)\n",
    " - Reducing\tthe\tconstraints\ton the model (e.g.,\treducing the regularization\thyperparameter)\n",
    "\n",
    "#### Stepping Back\n",
    "\n",
    "Big picture:\n",
    " - Machine Learning\tis about making\tmachines get better\tat some\ttask by\tlearning from data,\tinstead\tof having to explicitly\tcode rules.\n",
    " - There are many different types of ML\tsystems: supervised\tor not,\tbatch or online, instance-based\tor model-based,\tand so on.\n",
    " - In a\tML project you gather data in a training set, and you feed the training\tset\tto a learning algorithm. If\tthe\talgorithm is model-based, it tunes some\tparameters to fit the model\tto the training set (i.e., to make good predictions\ton the training set\titself), and then hopefully it will be able to make good predictions on\tnew\tcases as well. If the algorithm is instance-based, it just learns the examples by heart\tand\tuses a similarity measure to generalize to new instances.\n",
    " - The system will not perform well if your training set is too small, or if the data is not representative, noisy, or polluted\twith irrelevant features (garbage in, garbage out). Lastly, your model needs to be neither too simple (in which case it will underfit) nor too complex (in which case it will overfit).\n",
    " - Evaluate\tthe trained model, and fine-tune it if necessary.\n",
    " \n",
    "#### Testing and Validating\n",
    "\n",
    "Split data into _training set_ and _test set_. \n",
    "\n",
    "_Generalization error (out-of-sample error)_: the error rate on new cases.\n",
    "\n",
    "_Validation set_: to determine hyperparameter\n",
    "\n",
    "_Cross-validation_ (purpose: avoid “wasting” too much training data)\n",
    "<br>\n",
    "the training set is split into complementary subsets, and each model is trained against\ta different\tcombination\tof these subsets and validated against the remaining parts. Once the model type and hyperparameters\thave been selected,\ta final model is trained using these hyperparameters on the\tfull training set, and the generalized error is measured on\tthe\ttest set.\n",
    "\n",
    "_No Free Lunch_ (NFL) theorem\n",
    "<br>\n",
    "There is no model that is a\t_priori_ guaranteed to work better. (David Wolpert, 1996)\n",
    "\n",
    "In other words, if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and you evaluate only a few reasonable models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
